{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from metric_learn import LMNN\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "def kNN(k,query,qLabels,qID,gallery,gLabels,gID,metric):\n",
    "    G = [] #list of lists, #list = #query imgs, length of each list = #gallery imgs used\n",
    "    for i in range(len(query)):\n",
    "        #indices for gallery features to use\n",
    "        G.append([x for x in range(len(gallery)) if not(gLabels[x]==qLabels[i] and gID[x]==qID[i])]) \n",
    "    print('G')\n",
    "    \n",
    "    sorted_idx = []\n",
    "    for i in range(len(query)):\n",
    "        Dist = distance.cdist(np.reshape(query[i],(1,-1)),gallery[G[i]],metric = metric)\n",
    "        sorted_idx.append(np.argsort(Dist))\n",
    "    print('sorted_idx')\n",
    "\n",
    "    def accuracy(k):\n",
    "        NN = [arr[0,:k] for arr in sorted_idx]\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(len(query)):\n",
    "            usedLabels = gLabels[G[i]] # labels of gallery images used for each query image\n",
    "            if(qLabels[i] in usedLabels[NN[i]]): \n",
    "                sum += 1\n",
    "        acc = sum/len(query)\n",
    "        return acc\n",
    "    print('NN')\n",
    "    \n",
    "    if type(k) is list:\n",
    "        acc = []\n",
    "        for n in k:\n",
    "            acc.append(accuracy(n))\n",
    "    else:\n",
    "        acc = accuracy(k)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camID: (14096,)\n",
      "filelist: (14096,)\n",
      "gallery_idx: (5328,)\n",
      "labels: (14096,)\n",
      "query_idx: (1400,)\n",
      "train_idx: (7368,)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')\n",
    "camID = data['camId'].flatten()\n",
    "filelist = data['filelist'].flatten()\n",
    "gallery_idx = data['gallery_idx'].flatten()\n",
    "labels = data['labels'].flatten()\n",
    "query_idx = data['query_idx'].flatten()\n",
    "train_idx = data['train_idx'].flatten()\n",
    "\n",
    "print('camID:',camID.shape)\n",
    "print('filelist:',filelist.shape)\n",
    "print('gallery_idx:',gallery_idx.shape)\n",
    "print('labels:',labels.shape)\n",
    "print('query_idx:',query_idx.shape)\n",
    "print('train_idx:',train_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14096, 2048)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    features = loadmat('PR_data/features.mat')\n",
    "    features = features['features']\n",
    "except FileNotFoundError:\n",
    "    print('exception handling')\n",
    "    with open('PR_data/feature_data.json','r')as f: \n",
    "        features = json.load(f) \n",
    "        features = np.asarray(features) # each row is a feature (data instance) print(features.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "6601\n"
     ]
    }
   ],
   "source": [
    "train = features[train_idx-1]\n",
    "scalar = StandardScaler().fit(train)\n",
    "#train = StandardScaler().fit_transform(features[train_idx-1])\n",
    "train = scalar.transform(train)\n",
    "tLabels = labels[train_idx-1]\n",
    "#query = features[query_idx-1]\n",
    "query = scalar.transform(features[query_idx-1])\n",
    "qLabels = labels[query_idx-1]\n",
    "#gallery = features[gallery_idx-1]\n",
    "gallery = scalar.transform(features[gallery_idx-1])\n",
    "gLabels = labels[gallery_idx-1]\n",
    "tID = camID[train_idx-1]\n",
    "qID = camID[query_idx-1]\n",
    "gID = camID[gallery_idx-1]\n",
    "\n",
    "c = len(np.unique(tLabels)) #767\n",
    "print(c)\n",
    "\n",
    "print(len(train)-c) # 6601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapoint(object):\n",
    "    def __init__(self, features, label, cam_id):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.cam_id = cam_id\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, datapoint_list=None):\n",
    "        if datapoint_list:\n",
    "            self.datapoints = datapoint_list\n",
    "        else:\n",
    "            self.datapoints = []\n",
    "    \n",
    "    def features_array(self):\n",
    "        return np.array([datapoint.features for datapoint in self.datapoints])\n",
    "    def labels(self):\n",
    "        return np.array([datapoint.label for datapoint in self.datapoints])\n",
    "    def cam_ids(self):\n",
    "        return np.array([datapoint.cam_id for datapoint in self.datapoints])\n",
    "\n",
    "training_dataset = Dataset([Datapoint(train[i], tLabels[i], tID[i]) for i in range(len(train))])\n",
    "query_dataset = Dataset([Datapoint(query[i], qLabels[i], qID[i]) for i in range(len(query))])\n",
    "gallery_dataset = Dataset([Datapoint(gallery[i], gLabels[i], gID[i]) for i in range(len(gallery))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, training_dataset, query_dataset, gallery_dataset):\n",
    "    model = model.fit(training_dataset.features_array(), training_dataset.labels())\n",
    "    omega_train = model.transform(training_dataset.features_array())\n",
    "    omega_query = model.transform(query_dataset.features_array())\n",
    "    omega_gallery = model.transform(gallery_dataset.features_array())\n",
    "    \n",
    "    return model, omega_train, omega_query, omega_gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA + LMNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7368, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/metric_learn/lmnn.py:62: UserWarning: use_pca does nothing for the python_LMNN implementation\n",
      "  warnings.warn('use_pca does nothing for the python_LMNN implementation')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 67.46037793159485\n"
     ]
    }
   ],
   "source": [
    "M_pca = 512\n",
    "\n",
    "pca = PCA(n_components=M_pca)\n",
    "pca, omega_train, omega_q_pca, omega_g_pca = fit_model(pca, training_dataset, query_dataset, gallery_dataset)\n",
    "w_pca = pca.components_\n",
    "print(omega_train.shape)\n",
    "np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "start = time.time()\n",
    "lmnn = LMNN(k=5,use_pca = True)\n",
    "lmnn.fit(omega_train,training_dataset.labels())\n",
    "\n",
    "omega_query = lmnn.transform(omega_q_pca)\n",
    "omega_gallery = lmnn.transform(omega_g_pca)\n",
    "\n",
    "end = time.time()\n",
    "print('time',end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only LMNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "lmnn = LMNN(k=3,use_pca = True)\n",
    "lmnn.fit(training_dataset.features_array(),training_dataset.labels())\n",
    "\n",
    "omega_query = lmnn.transform(query_dataset.features_array())\n",
    "omega_gallery = lmnn.transform(gallery_dataset.features_array())\n",
    "\n",
    "end = time.time()\n",
    "print('time',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = kNN(k = [1,2,3,5,10,20,50,75,100],query=omega_query,qLabels=query_dataset.labels(),qID=qID,gallery=omega_gallery,\n",
    "          gLabels=gLabels,gID=gID,metric='euclidean')\n",
    "print('pca:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
