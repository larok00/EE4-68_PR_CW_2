{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from metric_learn import LMNN\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "def kNN(k,query,qLabels,qID,gallery,gLabels,gID,metric):\n",
    "    G = [] #list of lists, #list = #query imgs, length of each list = #gallery imgs used\n",
    "    for i in range(len(query)):\n",
    "        #indices for gallery features to use\n",
    "        G.append([x for x in range(len(gallery)) if not(gLabels[x]==qLabels[i] and gID[x]==qID[i])]) \n",
    "    print('G')\n",
    "    \n",
    "    sorted_idx = []\n",
    "    for i in range(len(query)):\n",
    "        Dist = distance.cdist(np.reshape(query[i],(1,-1)),gallery[G[i]],metric = metric)\n",
    "        sorted_idx.append(np.argsort(Dist))\n",
    "    print('sorted_idx')\n",
    "\n",
    "    def accuracy(k):\n",
    "        NN = [arr[0,:k] for arr in sorted_idx]\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(len(query)):\n",
    "            usedLabels = gLabels[G[i]] # labels of gallery images used for each query image\n",
    "            if(qLabels[i] in usedLabels[NN[i]]): \n",
    "                sum += 1\n",
    "        acc = sum/len(query)\n",
    "        return acc\n",
    "    print('NN')\n",
    "    \n",
    "    if type(k) is list:\n",
    "        acc = []\n",
    "        for n in k:\n",
    "            acc.append(accuracy(n))\n",
    "    else:\n",
    "        acc = accuracy(k)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camID: (14096,)\n",
      "filelist: (14096,)\n",
      "gallery_idx: (5328,)\n",
      "labels: (14096,)\n",
      "query_idx: (1400,)\n",
      "train_idx: (7368,)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')\n",
    "camID = data['camId'].flatten()\n",
    "filelist = data['filelist'].flatten()\n",
    "gallery_idx = data['gallery_idx'].flatten()\n",
    "labels = data['labels'].flatten()\n",
    "query_idx = data['query_idx'].flatten()\n",
    "train_idx = data['train_idx'].flatten()\n",
    "\n",
    "print('camID:',camID.shape)\n",
    "print('filelist:',filelist.shape)\n",
    "print('gallery_idx:',gallery_idx.shape)\n",
    "print('labels:',labels.shape)\n",
    "print('query_idx:',query_idx.shape)\n",
    "print('train_idx:',train_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14096, 2048)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    features = loadmat('PR_data/features.mat')\n",
    "    features = features['features']\n",
    "except FileNotFoundError:\n",
    "    print('exception handling')\n",
    "    with open('PR_data/feature_data.json','r')as f: \n",
    "        features = json.load(f) \n",
    "        features = np.asarray(features) # each row is a feature (data instance) print(features.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "700 700\n",
      "6601\n"
     ]
    }
   ],
   "source": [
    "train = features[train_idx-1]\n",
    "scalar = StandardScaler().fit(train)\n",
    "#train = StandardScaler().fit_transform(features[train_idx-1])\n",
    "train = scalar.transform(train)\n",
    "tLabels = labels[train_idx-1]\n",
    "#query = features[query_idx-1]\n",
    "query = scalar.transform(features[query_idx-1])\n",
    "qLabels = labels[query_idx-1]\n",
    "#gallery = features[gallery_idx-1]\n",
    "gallery = scalar.transform(features[gallery_idx-1])\n",
    "gLabels = labels[gallery_idx-1]\n",
    "tID = camID[train_idx-1]\n",
    "qID = camID[query_idx-1]\n",
    "gID = camID[gallery_idx-1]\n",
    "\n",
    "c = len(np.unique(tLabels)) #767\n",
    "print(c)\n",
    "\n",
    "num_clusters = len(np.unique(gLabels))\n",
    "print(num_clusters, len(np.unique(qLabels)))\n",
    "print(len(train)-c) # 6601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapoint(object):\n",
    "    def __init__(self, features, label, cam_id):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.cam_id = cam_id\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, datapoint_list=None):\n",
    "        if datapoint_list:\n",
    "            self.datapoints = datapoint_list\n",
    "        else:\n",
    "            self.datapoints = []\n",
    "    \n",
    "    def features_array(self):\n",
    "        return np.array([datapoint.features for datapoint in self.datapoints])\n",
    "    def labels(self):\n",
    "        return np.array([datapoint.label for datapoint in self.datapoints])\n",
    "    def cam_ids(self):\n",
    "        return np.array([datapoint.cam_id for datapoint in self.datapoints])\n",
    "\n",
    "training_dataset = Dataset([Datapoint(train[i], tLabels[i], tID[i]) for i in range(len(train))])\n",
    "query_dataset = Dataset([Datapoint(query[i], qLabels[i], qID[i]) for i in range(len(query))])\n",
    "gallery_dataset = Dataset([Datapoint(gallery[i], gLabels[i], gID[i]) for i in range(len(gallery))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, training_dataset, query_dataset, gallery_dataset):\n",
    "    model = model.fit(training_dataset.features_array(), training_dataset.labels())\n",
    "    omega_train = model.transform(training_dataset.features_array())\n",
    "    omega_query = model.transform(query_dataset.features_array())\n",
    "    omega_gallery = model.transform(gallery_dataset.features_array())\n",
    "    \n",
    "    return model, omega_train, omega_query, omega_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.utils.linear_assignment_ as la\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    " \n",
    "def best_map(l1, l2): #returns new cluster labels\n",
    "    \"\"\"\n",
    "    Permute labels of l2 to match l1 as much as possible\n",
    "    \"\"\"\n",
    "    #in this case l2 are the cluster labels, l1 are the data labels\n",
    "    if len(l1) != len(l2):\n",
    "        print(\"L1.shape must == L2.shape\")\n",
    "        exit(0)\n",
    " \n",
    "    label1 = np.unique(l1)\n",
    "    n_class1 = len(label1)\n",
    " \n",
    "    label2 = np.unique(l2)\n",
    "    n_class2 = len(label2)\n",
    " \n",
    "    n_class = max(n_class1, n_class2)\n",
    "    G = np.zeros((n_class, n_class))\n",
    " \n",
    "    for i in range(0, n_class1):\n",
    "        for j in range(0, n_class2):\n",
    "            ss = l1 == label1[i]\n",
    "            tt = l2 == label2[j]\n",
    "            G[i, j] = np.count_nonzero(ss & tt)\n",
    " \n",
    "    A = la.linear_assignment(-G)\n",
    " \n",
    "    new_l2 = np.zeros(l2.shape)\n",
    "    for i in range(0, n_class2):\n",
    "        new_l2[l2 == label2[A[i][1]]] = label1[A[i][0]]\n",
    "    return new_l2.astype(int)\n",
    " \n",
    " \n",
    "def evaluation(X_selected, n_clusters, y):\n",
    "    \"\"\"\n",
    "    This function calculates ARI, ACC and NMI of clustering results\n",
    " \n",
    "    Input\n",
    "    -----\n",
    "    X_selected: {numpy array}, shape (n_samples, n_selected_features}\n",
    "            input data on the selected features \n",
    "    n_clusters: {int}\n",
    "            number of clusters\n",
    "    y: {numpy array}, shape (n_samples,)\n",
    "            true labels\n",
    " \n",
    "    Output\n",
    "    ------\n",
    "    nmi: {float}\n",
    "        Normalized Mutual Information\n",
    "    acc: {float}\n",
    "        Accuracy\n",
    "    \"\"\"\n",
    "    k_means = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300,\n",
    "                     tol=0.0001, precompute_distances=True, verbose=0,\n",
    "                     random_state=None, copy_x=True, n_jobs=1)\n",
    " \n",
    "    k_means.fit(X_selected)\n",
    "    y_predict = k_means.labels_ #original cluster labels \n",
    "    \n",
    "    print(len(k_means.labels_))\n",
    " \n",
    "    # calculate NMI\n",
    "    nmi = normalized_mutual_info_score(y, y_predict) \n",
    " \n",
    "    # calculate ACC\n",
    "    y_permuted_predict = best_map(y, y_predict) # new cluster labels\n",
    "    acc = accuracy_score(y, y_permuted_predict) \n",
    " \n",
    "    return nmi, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "print(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5328\n"
     ]
    }
   ],
   "source": [
    "nmi,acc = evaluation(gallery_dataset.features_array(),num_clusters,gallery_dataset.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162185277915547 0.6739864864864865\n"
     ]
    }
   ],
   "source": [
    "print(nmi,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5328\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(gallery_dataset.features_array())\n",
    "print(len(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n",
      "sorted_idx\n",
      "NN\n",
      "pca: [0.43, 0.5157142857142857, 0.5685714285714286, 0.64, 0.7192857142857143, 0.7892857142857143, 0.8735714285714286, 0.8992857142857142, 0.9157142857142857]\n"
     ]
    }
   ],
   "source": [
    "acc = kNN(k = [1,2,3,5,10,20,50,75,100],query=omega_query,qLabels=query_dataset.labels(),qID=qID,gallery=omega_gallery,\n",
    "          gLabels=gLabels,gID=gID,metric='euclidean')\n",
    "print('pca:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
