{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camID: (14096,)\n",
      "filelist: (14096,)\n",
      "gallery_idx: (5328,)\n",
      "labels: (14096,)\n",
      "query_idx: (1400,)\n",
      "train_idx: (7368,)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')\n",
    "camID = data['camId'].flatten()\n",
    "filelist = data['filelist'].flatten()\n",
    "gallery_idx = data['gallery_idx'].flatten()\n",
    "labels = data['labels'].flatten()\n",
    "query_idx = data['query_idx'].flatten()\n",
    "train_idx = data['train_idx'].flatten()\n",
    "\n",
    "print('camID:',camID.shape)\n",
    "print('filelist:',filelist.shape)\n",
    "print('gallery_idx:',gallery_idx.shape)\n",
    "print('labels:',labels.shape)\n",
    "print('query_idx:',query_idx.shape)\n",
    "print('train_idx:',train_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   UNCOMMENT!!!!!!!!!!!!1\n",
    "with open('PR_data/feature_data.json','r')as f:\n",
    "    features = json.load(f)\n",
    "print(len(features), len(features[0]))\n",
    "\n",
    "features = np.asarray(features) # each row is a feature (data instance)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14096, 2048)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    features = loadmat('PR_data/features.mat')\n",
    "    features = features['features']\n",
    "except FileNotFoundError:\n",
    "    print('exception handling')\n",
    "    with open('PR_data/feature_data.json','r')as f: \n",
    "        features = json.load(f) \n",
    "        features = np.asarray(features) # each row is a feature (data instance) print(features.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "def kNN(k,query,qLabels,qID,gallery,gLabels,gID,metric):\n",
    "    G = [] #list of lists, #list = #query imgs, length of each list = #gallery imgs used\n",
    "    for i in range(len(query)):\n",
    "        #indices for gallery features to use\n",
    "        G.append([x for x in range(len(gallery)) if not(gLabels[x]==qLabels[i] and gID[x]==qID[i])]) \n",
    "    print('G')\n",
    "    \n",
    "    sorted_idx = []\n",
    "    for i in range(len(query)):\n",
    "        Dist = distance.cdist(np.reshape(query[i],(1,-1)),gallery[G[i]],metric = metric)\n",
    "        sorted_idx.append(np.argsort(Dist))\n",
    "    print('sorted_idx')\n",
    "\n",
    "    def accuracy(k):\n",
    "        NN = [arr[0,:k] for arr in sorted_idx]\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(len(query)):\n",
    "            usedLabels = gLabels[G[i]] # labels of gallery images used for each query image\n",
    "            if(qLabels[i] in usedLabels[NN[i]]): \n",
    "                sum += 1\n",
    "        acc = sum/len(query)\n",
    "        return acc\n",
    "    print('NN')\n",
    "    \n",
    "    if type(k) is list:\n",
    "        acc = []\n",
    "        for n in k:\n",
    "            acc.append(accuracy(n))\n",
    "    else:\n",
    "        acc = accuracy(k)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, training_dataset, query_dataset, gallery_dataset):\n",
    "    model = model.fit(training_dataset.features_array(), training_dataset.labels())\n",
    "    omega_query = model.transform(query_dataset.features_array())\n",
    "    omega_gallery = model.transform(gallery_dataset.features_array())\n",
    "    \n",
    "    return model, omega_query, omega_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "6601\n"
     ]
    }
   ],
   "source": [
    "train = features[train_idx-1]\n",
    "scalar = StandardScaler().fit(train)\n",
    "#train = StandardScaler().fit_transform(features[train_idx-1])\n",
    "train = scalar.transform(train)\n",
    "tLabels = labels[train_idx-1]\n",
    "#query = features[query_idx-1]\n",
    "query = scalar.transform(features[query_idx-1])\n",
    "qLabels = labels[query_idx-1]\n",
    "#gallery = features[gallery_idx-1]\n",
    "gallery = scalar.transform(features[gallery_idx-1])\n",
    "gLabels = labels[gallery_idx-1]\n",
    "tID = camID[train_idx-1]\n",
    "qID = camID[query_idx-1]\n",
    "gID = camID[gallery_idx-1]\n",
    "\n",
    "c = len(np.unique(tLabels)) #767\n",
    "print(c)\n",
    "\n",
    "print(len(train)-c) # 6601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapoint(object):\n",
    "    def __init__(self, features, label, cam_id):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.cam_id = cam_id\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, datapoint_list=None):\n",
    "        if datapoint_list:\n",
    "            self.datapoints = datapoint_list\n",
    "        else:\n",
    "            self.datapoints = []\n",
    "    \n",
    "    def features_array(self):\n",
    "        return np.array([datapoint.features for datapoint in self.datapoints])\n",
    "    def labels(self):\n",
    "        return np.array([datapoint.label for datapoint in self.datapoints])\n",
    "    def cam_ids(self):\n",
    "        return np.array([datapoint.cam_id for datapoint in self.datapoints])\n",
    "\n",
    "training_dataset = Dataset([Datapoint(train[i], tLabels[i], tID[i]) for i in range(len(train))])\n",
    "query_dataset = Dataset([Datapoint(query[i], qLabels[i], qID[i]) for i in range(len(query))])\n",
    "gallery_dataset = Dataset([Datapoint(gallery[i], gLabels[i], gID[i]) for i in range(len(gallery))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set\n",
    "np.random.seed(seed = 342)\n",
    "valLabels = np.random.choice(np.unique(tLabels),115,replace=False)\n",
    "validation_subset = Dataset()\n",
    "training_subset = Dataset() #6251 features\n",
    "for x in training_dataset.datapoints:\n",
    "    if x.label in valLabels:\n",
    "        validation_subset.datapoints.append(x)\n",
    "    else:\n",
    "        training_subset.datapoints.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1533 5835\n",
      "230 887\n"
     ]
    }
   ],
   "source": [
    "#split validation_subset into validation_query and validation_gallery\n",
    "def gallery_query_split(dataset):\n",
    "    random.seed(3)\n",
    "    shuffled = dataset\n",
    "    random.shuffle(shuffled.datapoints)\n",
    "\n",
    "    query = Dataset()\n",
    "    gallery = Dataset()\n",
    "    query_set = set()\n",
    "    for x in shuffled.datapoints:\n",
    "        if (x.label, x.cam_id) in query_set:\n",
    "            gallery.datapoints.append(x)\n",
    "        else:\n",
    "            query.datapoints.append(x)\n",
    "            query_set.add((x.label, x.cam_id))\n",
    "\n",
    "    print(len(query.datapoints),len(gallery.datapoints))\n",
    "    return query, gallery\n",
    "\n",
    "training_query, training_gallery = gallery_query_split(training_dataset)\n",
    "validation_query, validation_gallery = gallery_query_split(validation_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2048)\n",
      "G\n",
      "sorted_idx\n",
      "NN\n",
      "pca training : [0.9895629484670581, 0.9895629484670581, 0.9895629484670581, 0.9902152641878669, 0.9902152641878669, 0.9902152641878669, 0.9908675799086758, 0.9915198956294846, 0.9915198956294846]\n"
     ]
    }
   ],
   "source": [
    "M_pca = 1000\n",
    "\n",
    "pca = PCA(n_components=M_pca)\n",
    "pca, omega_q_pca, omega_g_pca = fit_model(pca, training_subset, training_query, training_gallery)\n",
    "w_pca = pca.components_\n",
    "print(w_pca.shape)\n",
    "np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "acc_pca = kNN(k = [1,2,3,5,10,20,50,75,100],query=omega_q_pca,qLabels=training_query.labels(),\n",
    "              qID=training_query.cam_ids(),gallery=omega_g_pca,\n",
    "              gLabels=training_gallery.labels(),gID=training_gallery.cam_ids(),metric='euclidean')\n",
    "print('pca training :',acc_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2048) (230, 1000)\n",
      "G\n",
      "sorted_idx\n",
      "NN\n",
      "pca validation : [0.9956521739130435, 0.9956521739130435, 0.9956521739130435, 0.9956521739130435, 0.9956521739130435, 0.9956521739130435, 0.9956521739130435, 0.9956521739130435, 0.9956521739130435]\n"
     ]
    }
   ],
   "source": [
    "M_pca = 1000\n",
    "\n",
    "pca = PCA(n_components=M_pca)\n",
    "\n",
    "pca, omega_q_pca, omega_g_pca = fit_model(pca, training_subset, validation_query, validation_gallery)\n",
    "w_pca = pca.components_\n",
    "print(w_pca.shape, omega_q_pca.shape)\n",
    "np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "acc_pca = kNN(k = [1,2,3,5,10,20,50,75,100],query=omega_q_pca,qLabels=validation_query.labels(),\n",
    "              qID=validation_query.cam_ids(),gallery=omega_g_pca,\n",
    "              gLabels=validation_gallery.labels(),gID=validation_gallery.cam_ids(),metric='euclidean')\n",
    "print('pca validation :',acc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2048)\n",
      "0.7723529685691517\n",
      "(1400, 300) (5328, 300)\n",
      "G\n",
      "sorted_idx\n",
      "NN\n",
      "lda: [0.4278571428571429, 0.4957142857142857, 0.5535714285714286, 0.6257142857142857, 0.7107142857142857, 0.7921428571428571, 0.8678571428571429, 0.8985714285714286, 0.9185714285714286]\n"
     ]
    }
   ],
   "source": [
    "M_lda = 300\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "lda, omega_q_lda, omega_g_lda = fit_model(lda, training_dataset, query_dataset, gallery_dataset) #(1400,Mlda),(5328,M_pca)\n",
    "w_lda = lda.coef_[:M_lda,:]\n",
    "print(w_lda.shape)\n",
    "print(np.sum(lda.explained_variance_ratio_))\n",
    "print(omega_q_lda.shape,omega_g_lda.shape)\n",
    "\n",
    "acc_lda = kNN(k = [1,2,3,5,10,20,50,75,100],query=omega_q_lda,qLabels=qLabels,qID=qID,gallery=omega_g_lda,\n",
    "          gLabels=gLabels,gID=gID,metric='euclidean')\n",
    "print('lda:',acc_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
