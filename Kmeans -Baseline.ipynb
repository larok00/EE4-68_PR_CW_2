{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from metric_learn import LMNN\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.utils.linear_assignment_ as la\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camID: (14096,)\n",
      "filelist: (14096,)\n",
      "gallery_idx: (5328,)\n",
      "labels: (14096,)\n",
      "query_idx: (1400,)\n",
      "train_idx: (7368,)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('PR_data/cuhk03_new_protocol_config_labeled.mat')\n",
    "camID = data['camId'].flatten()\n",
    "filelist = data['filelist'].flatten()\n",
    "gallery_idx = data['gallery_idx'].flatten()\n",
    "labels = data['labels'].flatten()\n",
    "query_idx = data['query_idx'].flatten()\n",
    "train_idx = data['train_idx'].flatten()\n",
    "\n",
    "print('camID:',camID.shape)\n",
    "print('filelist:',filelist.shape)\n",
    "print('gallery_idx:',gallery_idx.shape)\n",
    "print('labels:',labels.shape)\n",
    "print('query_idx:',query_idx.shape)\n",
    "print('train_idx:',train_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14096, 2048)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    features = loadmat('PR_data/features.mat')\n",
    "    features = features['features']\n",
    "except FileNotFoundError:\n",
    "    print('exception handling')\n",
    "    with open('PR_data/feature_data.json','r')as f: \n",
    "        features = json.load(f) \n",
    "        features = np.asarray(features) # each row is a feature (data instance) print(features.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "700 700\n",
      "6601\n"
     ]
    }
   ],
   "source": [
    "train = features[train_idx-1]\n",
    "scalar = StandardScaler().fit(train)\n",
    "#train = StandardScaler().fit_transform(features[train_idx-1])\n",
    "train = scalar.transform(train)\n",
    "tLabels = labels[train_idx-1]\n",
    "#query = features[query_idx-1]\n",
    "query = scalar.transform(features[query_idx-1])\n",
    "qLabels = labels[query_idx-1]\n",
    "#gallery = features[gallery_idx-1]\n",
    "gallery = scalar.transform(features[gallery_idx-1])\n",
    "gLabels = labels[gallery_idx-1]\n",
    "tID = camID[train_idx-1]\n",
    "qID = camID[query_idx-1]\n",
    "gID = camID[gallery_idx-1]\n",
    "\n",
    "c = len(np.unique(tLabels)) #767\n",
    "print(c)\n",
    "\n",
    "num_clusters = len(np.unique(gLabels))\n",
    "print(num_clusters, len(np.unique(qLabels)))\n",
    "print(len(train)-c) # 6601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapoint(object):\n",
    "    def __init__(self, features, label, cam_id):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.cam_id = cam_id\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, datapoint_list=None):\n",
    "        if datapoint_list:\n",
    "            self.datapoints = datapoint_list\n",
    "        else:\n",
    "            self.datapoints = []\n",
    "    \n",
    "    def features_array(self):\n",
    "        return np.array([datapoint.features for datapoint in self.datapoints])\n",
    "    def labels(self):\n",
    "        return np.array([datapoint.label for datapoint in self.datapoints])\n",
    "    def cam_ids(self):\n",
    "        return np.array([datapoint.cam_id for datapoint in self.datapoints])\n",
    "\n",
    "training_dataset = Dataset([Datapoint(train[i], tLabels[i], tID[i]) for i in range(len(train))])\n",
    "query_dataset = Dataset([Datapoint(query[i], qLabels[i], qID[i]) for i in range(len(query))])\n",
    "gallery_dataset = Dataset([Datapoint(gallery[i], gLabels[i], gID[i]) for i in range(len(gallery))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, training_dataset, query_dataset, gallery_dataset):\n",
    "    model = model.fit(training_dataset.features_array(), training_dataset.labels())\n",
    "    omega_train = model.transform(training_dataset.features_array())\n",
    "    omega_query = model.transform(query_dataset.features_array())\n",
    "    omega_gallery = model.transform(gallery_dataset.features_array())\n",
    "    \n",
    "    return model, omega_train, omega_query, omega_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_map(l1, l2): #returns new cluster labels\n",
    "    \"\"\"\n",
    "    Permute labels of l2 to match l1 as much as possible\n",
    "    \"\"\"\n",
    "    #in this case label2 are the cluster labels, label1 are the data labels\n",
    "    if len(l1) != len(l2):\n",
    "        print(\"L1.shape must == L2.shape\")\n",
    "        exit(0)\n",
    " \n",
    "    label1 = np.unique(l1)\n",
    "    n_class1 = len(label1)\n",
    " \n",
    "    label2 = np.unique(l2)\n",
    "    n_class2 = len(label2)\n",
    " \n",
    "    n_class = max(n_class1, n_class2)\n",
    "    G = np.zeros((n_class, n_class))\n",
    " \n",
    "    for i in range(0, n_class1):\n",
    "        for j in range(0, n_class2):\n",
    "            ss = l1 == label1[i]\n",
    "            tt = l2 == label2[j]\n",
    "            G[i, j] = np.count_nonzero(ss & tt)\n",
    " \n",
    "    A = la.linear_assignment(-G)\n",
    "    print('A',A.shape,'\\n,')\n",
    "    print(A)\n",
    " \n",
    "    new_l2 = np.zeros(l2.shape) \n",
    "    for i in range(0, n_class2): #target labels\n",
    "        new_l2[l2 == label2[A[i][1]]] = label1[A[i][0]] # \n",
    "    return new_l2.astype(int)\n",
    " \n",
    " \n",
    "def evaluation(X_selected,n_clusters,y):\n",
    "    \"\"\"\n",
    "    This function calculates ARI, ACC and NMI of clustering results\n",
    " \n",
    "    Input\n",
    "    -----\n",
    "    X_selected: {numpy array}, shape (n_samples, n_selected_features}\n",
    "            input data on the selected features \n",
    "    n_clusters: {int}\n",
    "            number of clusters\n",
    "    y: {numpy array}, shape (n_samples,)\n",
    "            true labels\n",
    " \n",
    "    Output\n",
    "    ------\n",
    "    nmi: {float}\n",
    "        Normalized Mutual Information\n",
    "    acc: {float}\n",
    "        Accuracy\n",
    "    \"\"\"\n",
    "    k_means = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300,\n",
    "                     tol=0.0001, precompute_distances=True, verbose=0,\n",
    "                     random_state=0, copy_x=True, n_jobs=1)\n",
    " \n",
    "    k_means.fit(X_selected)\n",
    "    y_predict = k_means.labels_ #original cluster labels assigned to the gallery_features\n",
    "    print('y_predict',len(y_predict),len(np.unique(y_predict)))\n",
    " \n",
    "    # calculate NMI\n",
    "    nmi = normalized_mutual_info_score(y, y_predict) # y= gallery_labels , y_predict = cluster labels\n",
    "    h_score = homogeneity_score(y, y_predict) #each cluster only contains members of a single class\n",
    "    c_score = completeness_score(y, y_predict) #all members of a given class are assigned to the same cluster\n",
    "    print('homogenity:',h_score,'completeness:',c_score)\n",
    "    \n",
    "    # calculate ACC\n",
    "    y_permuted_predict = best_map(y, y_predict) # y_permuted_predict = new cluster labels\n",
    "    print('y_permuted_predict',len(y_permuted_predict),len(np.unique(y_permuted_predict)))\n",
    "    acc = accuracy_score(y, y_permuted_predict) #returns the number of correctly classified samples\n",
    "    #i.e. checks if the cluster labels match the labels assigned to the gallery_fectures \n",
    "    \n",
    "    # want to remap clustercentre labels\n",
    "    centers = k_means.cluster_centers_\n",
    "    \n",
    "    u, idx = np.unique(y_predict, return_index=True)\n",
    "    center_labels = y_permuted_predict[idx]\n",
    "    print(len(center_labels), len(np.unique(center_labels)))\n",
    "    \n",
    "    return nmi, acc, centers, center_labels,y_permuted_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict 5328 700\n",
      "homogenity: 0.9132354526702438 completeness: 0.9238192959588092\n",
      "A (700, 2) \n",
      ",\n",
      "[[  0 582]\n",
      " [  1 131]\n",
      " [  2 554]\n",
      " ...\n",
      " [697 196]\n",
      " [698 174]\n",
      " [699  60]]\n",
      "y_permuted_predict 5328 700\n",
      "700 700\n",
      "time: 185.5173168182373\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nmi,acc,centers,center_labels,gallery_clustered = evaluation(gallery_dataset.features_array(),num_clusters,gallery_dataset.labels())\n",
    "end = time.time()\n",
    "print('time:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 700)\n",
      "sorted_idx (1400, 700) [[661 293 135 ... 173 681 550]\n",
      " [582 293 661 ... 550 486 396]\n",
      " [131 621 102 ... 154 678 562]\n",
      " ...\n",
      " [588 174 453 ... 494 681 638]\n",
      " [ 60 120 350 ... 347 638 681]\n",
      " [ 60  76 216 ... 638 347 349]]\n"
     ]
    }
   ],
   "source": [
    "Dist = distance.cdist(query_dataset.features_array(),centers,metric = 'euclidean')\n",
    "print(Dist.shape)\n",
    "sorted_idx = np.argsort(Dist)\n",
    "print('sorted_idx',sorted_idx.shape,sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1) [[661]\n",
      " [582]\n",
      " [131]\n",
      " ...\n",
      " [588]\n",
      " [ 60]\n",
      " [ 60]]\n",
      "[1426]\n",
      "acc 68.42857142857143\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "NN = sorted_idx[:,:k]\n",
    "print(NN.shape,NN)\n",
    "print(center_labels[NN[0]])\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(query_dataset.features_array())):\n",
    "    if(query_dataset.labels()[i] in center_labels[NN[i]]): #if there is a match\n",
    "        sum += 1\n",
    "acc = sum/len(query)\n",
    "print('acc',acc*100)\n",
    "print('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
